inputs = [1,2,3,2.5] #unique inputs
#adding extra unique weight sets
weights1 = [0.2,0.8,-0.5,1.0]
weights2 = [0.5, -0.91, 0.26, -0.5]
weights3 = [-.26, -0.27, 0.17, 0.87]
#adding extra unique biases
bias1 = 2 
bias2 = 3
bias3 = 0.5
#pretty much the output for the neural network for now
output = [inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias,
          inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias,
          inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias
          ]
print(output) 


# the input values can be either just values or the layer
# the weights are given for the synopsys
# the bias is given for the neuron
# we are going to use 4 inputs and 3 neurons, that means there are going to be 3 unique weight sets, and each weight set is going to have 4 unique values, we are going to need unique biases 

=====================================

inputs = [1,2,3,2.5] #unique inputs
#adding extra unique weight sets
#adding extra unique biases
#using list of lists
weights = [[0.2,0.8,-0.5,1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-.26, -0.27, 0.17, 0.87]
           ]
biases = [2,3,0.5]
#pretty much the output for the neural network for now
# using loop structure to simplify the calculation
layer_outputs = []
for neuron_weights, neuron_bias in zip(weights, biases): # zip combines 2 lists into a list of lists
    neuron_output = 0
    for n_input , weight in zip(inputs,neuron_weights):
        neuron_output += n_input*weight
    neuron_output += neuron_bias
    layer_outputs.append(neuron_output)
print(layer_outputs)
## this for loop will be transitioned by using numpys and all


# the input values can be either just values or the layer
# the weights are given for the synopsys
# the bias is given for the neuron
# we are going to use 4 inputs and 3 neurons, that means there are going to be 3 unique weight sets, and each weight set is going to have 4 unique values, we are going to need unique biases 

# the next steps will be working the DOT PRODUCT
    #--code --> ditching the repetative code 
    #--weight and bias
    #--shape
    #--dot product

==========================================

inputs = [1,2,3,2.5] #unique inputs
#adding extra unique weight sets
#adding extra unique biases
#using list of lists
weights = [[0.2,0.8,-0.5,1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-.26, -0.27, 0.17, 0.87]
           ]
biases = [2,3,0.5]
#pretty much the output for the neural network for now
# using loop structure to simplify the calculation
layer_outputs = []
for neuron_weights, neuron_bias in zip(weights, biases): # zip combines 2 lists into a list of lists
    neuron_output = 0
    for n_input , weight in zip(inputs,neuron_weights):
        neuron_output += n_input*weight
    neuron_output += neuron_bias
    layer_outputs.append(neuron_output)
print(layer_outputs)
## this for loop will be transitioned by using numpys and all


# the input values can be either just values or the layer
# the weights are given for the synopsys
# the bias is given for the neuron
# we are going to use 4 inputs and 3 neurons, that means there are going to be 3 unique weight sets, and each weight set is going to have 4 unique values, we are going to need unique biases 

# the next steps will be working the DOT PRODUCT
    #--code --> ditching the repetative code 
    #--weight and bias
    #--shape
    #--dot product

# NOTES: weights and biases are used to tune the outcome 

# the concept of SHAPE
# shape is basically at each dimension, what's the size of 
# that dimension, suppose we have got a list of 4 elements : 
# list : [1,5,6,2] ; shape is (4,) ; type : 1Darray, vector
# list of list --> 2Darray, matrix
# tensor is an object that can be represented as an object that can be represented as an array
# we want to multiply the weights and inputs


=================================================


#using numpy to make the calculations
#using numpy will eliminate the use of for loops

import numpy as np
inputs = [1,2,3,2.5]
weights = [[0.2,0.8,-0.5,1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-.26, -0.27, 0.17, 0.87]]
biases = [2,3,0.5]
output = np.dot(weights,inputs) + biases
print(output)


# if we use np.dot(inputs, weights) --> error
# adjusting weights and biases can give us completely different


======================================================

#using numpy to make the calculations
#using numpy will eliminate the use of for loops

import numpy as np
inputs = [[1,2,3,2.5],
          [2.0,5.0,-1.0,2.0],
          [-1.5,2.7,3.3,-0.8]
          ] #--> features from a single sample
weights = [[0.2,0.8,-0.5,1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-.26, -0.27, 0.17, 0.87]]
biases = [2,3,0.5]
output = np.dot(weights,inputs) + biases
print(output)


# if we use np.dot(inputs, weights) --> error
# adjusting weights and biases can give us completely different and they are associated with individual neurons

since here the inputs matrix has 3rows and 4cols, the weights matrix has 4rows and 3 cols, there the dot product will give us an error.

========================================================
-->TO ADD FROM HERE

to fix the error we are going to use the transpose method
we are going to transpose the weight array, using the numpy method. So first we are going to convert the weights array to numpy array and then use .T to make it into a transpose

#using numpy to make the calculations
#using numpy will eliminate the use of for loops

import numpy as np
inputs = [[1,2,3,2.5],
          [2.0,5.0,-1.0,2.0],
          [-1.5,2.7,3.3,-0.8]
          ] #--> features from a single sample
weights = [[0.2,0.8,-0.5,1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-.26, -0.27, 0.17, 0.87]]
biases = [2,3,0.5]
output = np.dot(inputs, np.array(weights).T) + biases
print(output)


# if we use np.dot(inputs, weights) --> error
# adjusting weights and biases can give us completely different and they are associated with individual neurons

===========================================================

we are going to add the new layer

#using numpy to make the calculations
#using numpy will eliminate the use of for loops

import numpy as np
inputs = [[1,2,3,2.5],
          [2.0,5.0,-1.0,2.0],
          [-1.5,2.7,3.3,-0.8]
          ] #--> features from a single sample
weights = [[0.2,0.8,-0.5,1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-.26, -0.27, 0.17, 0.87]]
biases = [2,3,0.5]

weights2 = [[0.1,-0.14,0.5],
           [-0.5,0.12,-0.33],
           [-0.44,0.73,-0.13]]
biases2 = [-1,2,-0.5]

layer1_outputs = np.dot(inputs, np.array(weights).T) + biases
layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2
print(layer2_outputs)



# if we use np.dot(inputs, weights) --> error
# adjusting weights and biases can give us completely different and they are associated with individual neurons

#the layer1_output is the output for layer 1 which is then the input of layer2

===================================================

now we are convert all these to objects
converting the object of layers to objects

import numpy as np
np.random.seed(0)
X = [[1,2,3,2.5],
    [2.0,5.0,-1.0,2.0],
    [-1.5,2.7,3.3,-0.8]
          ] #--> features from a single sample

#gonna use oops
class Layer_Dense:
    def __init__(self, n_inputs, n_neurons):
        self.weights = 0.10*np.random.randn(n_inputs, n_neurons) # we are gonna pass the shape inside this paranthesis, we need to know kind of 2 things, whats the size of the input coming in and how many neurons are we gonna have
        self.biases = np.zeros((1, n_neurons)) #since the biases is a 1d array of the number of neurons
    def forward(self):
        pass
# in the case of neural networks there are some ways we are gonna initialise a layer, first is we are gonna have to train the model that we have saved and we want to load in that model 


