inputs = [1,2,3,2.5] #unique inputs
#adding extra unique weight sets
weights1 = [0.2,0.8,-0.5,1.0]
weights2 = [0.5, -0.91, 0.26, -0.5]
weights3 = [-.26, -0.27, 0.17, 0.87]
#adding extra unique biases
bias1 = 2 
bias2 = 3
bias3 = 0.5
#pretty much the output for the neural network for now
output = [inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias,
          inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias,
          inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias
          ]
print(output) 


# the input values can be either just values or the layer
# the weights are given for the synopsys
# the bias is given for the neuron
# we are going to use 4 inputs and 3 neurons, that means there are going to be 3 unique weight sets, and each weight set is going to have 4 unique values, we are going to need unique biases 

=====================================

inputs = [1,2,3,2.5] #unique inputs
#adding extra unique weight sets
#adding extra unique biases
#using list of lists
weights = [[0.2,0.8,-0.5,1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-.26, -0.27, 0.17, 0.87]
           ]
biases = [2,3,0.5]
#pretty much the output for the neural network for now
# using loop structure to simplify the calculation
layer_outputs = []
for neuron_weights, neuron_bias in zip(weights, biases): # zip combines 2 lists into a list of lists
    neuron_output = 0
    for n_input , weight in zip(inputs,neuron_weights):
        neuron_output += n_input*weight
    neuron_output += neuron_bias
    layer_outputs.append(neuron_output)
print(layer_outputs)
## this for loop will be transitioned by using numpys and all


# the input values can be either just values or the layer
# the weights are given for the synopsys
# the bias is given for the neuron
# we are going to use 4 inputs and 3 neurons, that means there are going to be 3 unique weight sets, and each weight set is going to have 4 unique values, we are going to need unique biases 

# the next steps will be working the DOT PRODUCT
    #--code --> ditching the repetative code 
    #--weight and bias
    #--shape
    #--dot product

==========================================

inputs = [1,2,3,2.5] #unique inputs
#adding extra unique weight sets
#adding extra unique biases
#using list of lists
weights = [[0.2,0.8,-0.5,1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-.26, -0.27, 0.17, 0.87]
           ]
biases = [2,3,0.5]
#pretty much the output for the neural network for now
# using loop structure to simplify the calculation
layer_outputs = []
for neuron_weights, neuron_bias in zip(weights, biases): # zip combines 2 lists into a list of lists
    neuron_output = 0
    for n_input , weight in zip(inputs,neuron_weights):
        neuron_output += n_input*weight
    neuron_output += neuron_bias
    layer_outputs.append(neuron_output)
print(layer_outputs)
## this for loop will be transitioned by using numpys and all


# the input values can be either just values or the layer
# the weights are given for the synopsys
# the bias is given for the neuron
# we are going to use 4 inputs and 3 neurons, that means there are going to be 3 unique weight sets, and each weight set is going to have 4 unique values, we are going to need unique biases 

# the next steps will be working the DOT PRODUCT
    #--code --> ditching the repetative code 
    #--weight and bias
    #--shape
    #--dot product

# NOTES: weights and biases are used to tune the outcome 

# the concept of SHAPE
# shape is basically at each dimension, what's the size of 
# that dimension, suppose we have got a list of 4 elements : 
# list : [1,5,6,2] ; shape is (4,) ; type : 1Darray, vector
# list of list --> 2Darray, matrix
# tensor is an object that can be represented as an object that can be represented as an array
# we want to multiply the weights and inputs


=================================================


#using numpy to make the calculations
#using numpy will eliminate the use of for loops

import numpy as np
inputs = [1,2,3,2.5]
weights = [[0.2,0.8,-0.5,1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-.26, -0.27, 0.17, 0.87]]
biases = [2,3,0.5]
output = np.dot(weights,inputs) + biases
print(output)


# if we use np.dot(inputs, weights) --> error
# adjusting weights and biases can give us completely different


======================================================

#using numpy to make the calculations
#using numpy will eliminate the use of for loops

import numpy as np
inputs = [[1,2,3,2.5],
          [2.0,5.0,-1.0,2.0],
          [-1.5,2.7,3.3,-0.8]
          ] #--> features from a single sample
weights = [[0.2,0.8,-0.5,1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-.26, -0.27, 0.17, 0.87]]
biases = [2,3,0.5]
output = np.dot(weights,inputs) + biases
print(output)


# if we use np.dot(inputs, weights) --> error
# adjusting weights and biases can give us completely different and they are associated with individual neurons

since here the inputs matrix has 3rows and 4cols, the weights matrix has 4rows and 3 cols, there the dot product will give us an error.

========================================================
to fix the error we are going to use the transpose method
we are going to transpose the weight array, using the numpy method. So first we are going to convert the weights array to numpy array and then use .T to make it into a transpose

#using numpy to make the calculations
#using numpy will eliminate the use of for loops

import numpy as np
inputs = [[1,2,3,2.5],
          [2.0,5.0,-1.0,2.0],
          [-1.5,2.7,3.3,-0.8]
          ] #--> features from a single sample
weights = [[0.2,0.8,-0.5,1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-.26, -0.27, 0.17, 0.87]]
biases = [2,3,0.5]
output = np.dot(inputs, np.array(weights).T) + biases
print(output)


# if we use np.dot(inputs, weights) --> error
# adjusting weights and biases can give us completely different and they are associated with individual neurons

===========================================================

we are going to add the new layer

#using numpy to make the calculations
#using numpy will eliminate the use of for loops

import numpy as np
inputs = [[1,2,3,2.5],
          [2.0,5.0,-1.0,2.0],
          [-1.5,2.7,3.3,-0.8]
          ] #--> features from a single sample
weights = [[0.2,0.8,-0.5,1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-.26, -0.27, 0.17, 0.87]]
biases = [2,3,0.5]

weights2 = [[0.1,-0.14,0.5],
           [-0.5,0.12,-0.33],
           [-0.44,0.73,-0.13]]
biases2 = [-1,2,-0.5]

layer1_outputs = np.dot(inputs, np.array(weights).T) + biases
layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2
print(layer2_outputs)



# if we use np.dot(inputs, weights) --> error
# adjusting weights and biases can give us completely different and they are associated with individual neurons

#the layer1_output is the output for layer 1 which is then the input of layer2

===================================================

now we are convert all these to objects
converting the object of layers to objects

import numpy as np
np.random.seed(0)
X = [[1,2,3,2.5],
    [2.0,5.0,-1.0,2.0],
    [-1.5,2.7,3.3,-0.8]
          ] #--> features from a single sample

#gonna use oops
class Layer_Dense:
    def __init__(self, n_inputs, n_neurons):
        self.weights = 0.10*np.random.randn(n_inputs, n_neurons) # we are gonna pass the shape inside this parenthesis, we need to know kind of 2 things, whats the size of the input coming in and how many neurons are we gonna have
        self.biases = np.zeros((1, n_neurons)) #since the biases is a 1d array of the number of neurons
    def forward(self, inputs):
        self.output = np.dot(inputs, self.weights)+self.biases
# in the case of neural networks there are some ways we are gonna initialise a layer, first is we are gonna have to train the model that we have saved and we want to load in that model 
# the output of the first layer is the input of the second layer
layer1 = Layer_Dense(4,5) #4--> input size ; 5 is the random value
layer2 = Layer_Dense(5,2) #5--> output of one layer is input of another
layer1.forward(X)
print(layer1.output)
layer2.forward(layer1.output)
print(layer2.output)

======================================================

Will be coding up an activation function, can be using a varity of functions the are examples that can be used, 
    -- step function == if x>0 , y==1 else y==0
    -- we will be using the step function as an activation function
    -- working principle :
        each neuron in the hidden layer and the output layer will have this activation function and this comes into play when we are using the inputs, the weights and the bias, when we tweak the weights and biases, the activation function will always give values between 0 and 1
    -- next type of function is the sigmoid function, the basic reason of using a sigmoid function, is that it gives us a more granular output, which tells us what is the impact of the weights and the biases on the activation function
    --next type of function is the rectifier function, which states that if x>0 --> y==x else y==0, better than sigmoid function

    ## why does the rectifier function work as an activation function==> watch the source again
    ## gonna use the nnfs package to install that --> pip install nnfs

import numpy as np
import nnfs
from nnfs.datasets import spiral_data

nnfs.init()  # Setting up a default datatype for the numpy use

X = [[1,2,3,2.5],
    [2.0,5.0,-1.0,2.0],
    [-1.5,2.7,3.3,-0.8]]

X, y = spiral_data(100, 3)  # 100 feature sets of 3 classes

# Define the Layer_Dense class
class Layer_Dense:
    def __init__(self, n_inputs, n_neurons):
        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)
        self.biases = np.zeros((1, n_neurons))

    def forward(self, inputs):
        self.output = np.dot(inputs, self.weights) + self.biases

# Define the Activation_ReLU class
class Activation_ReLU:
    def forward(self, inputs):
        self.output = np.maximum(0, inputs)

# Initialize the layers
layer1 = Layer_Dense(2, 5)  # 2 input features (matches the dataset), 5 neurons
activation1 = Activation_ReLU()

# Perform a forward pass
layer1.forward(X)
activation1.forward(layer1.output)

# Print the output of the first layer
#print(layer1.output)
activation1.forward(layer1.output)
print(activation1.output)
## PART 5 IS DONE


==================================================
#coding up the softmax function

steps:
1) exponatiating every output value of the output layer
2) then normalizing the exponatiated values

-----------------------------------------
## without using numpy
import math
layer_outputs = [4.8,1.21,2.385]
E = math.e
exp_values = []
for output in layer_outputs:
    exp_values.append(E**output)
print(exp_values)

## next we will be normalizing the exponitiated values
norm_base = sum(exp_values)
norm_values = []
for value in exp_values:
    norm_values.append(value/norm_base)
print(norm_values)
print(sum(norm_values)) ##this will round up to 1
----------------------------------------------

------------------------------------------------
## with using numpy(using single layer of output)
import numpy as np
layer_outputs = [4.8,1.21,2.385]
exp_values = np.exp(layer_outputs)
norm_values = exp_values/np.sum(exp_values)
print(norm_values)
print(sum(norm_values))
-------------------------------------------------

-------------------------------------------------
## with using numpy(using multiple layer of output)
import numpy as np
import nnfs
nnfs.init()
layer_outputs = [[4.8,1.21,2.385],
                 [8.9, -1.81, 0.2],
                 [1.41, 1.051, 0.026]]
exp_values = np.exp(layer_outputs)
norm_values = exp_values/np.sum(exp_values, axis=1,keepdims=True) 
##np.sum(exp_values, axis=1, keepdims=True) 
##axis=1 --> for row wise sums, keepdims=True --> for printing the sums output in the layer_outputs shape
print(norm_values)


===============================================
adding the concept of softmax function to the neural network code

import numpy as np
import nnfs
from nnfs.datasets import spiral_data

nnfs.init()  # Setting up a default datatype for the numpy use

# Define the Layer_Dense class
class Layer_Dense:
    def __init__(self, n_inputs, n_neurons):
        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)
        self.biases = np.zeros((1, n_neurons))

    def forward(self, inputs):
        self.output = np.dot(inputs, self.weights) + self.biases

# Define the Activation_ReLU class
class Activation_ReLU:
    def forward(self, inputs):
        self.output = np.maximum(0, inputs)

class Activation_Softmax:
    def forward(self, inputs):
        exp_values = np.exp(inputs-np.max(inputs,axis=1,keepdims=True)) #avoiding overflow since we are using the exponential function
        probabilities = exp_values/np.sum(exp_values,axis=1,keepdims=True)
        self.output = probabilities

X,y = spiral_data(samples=100,classes=3)
dense1 = Layer_Dense(2,3)
activation1 = Activation_ReLU()

dense2 = Layer_Dense(3,3)
activation2 = Activation_Softmax()

dense1.forward(X)
activation1.forward(dense1.output)

dense2.forward(activation1.output)
activation2.forward(dense2.output)

print(activation2.output[:5])
## GONNA HAVE TO UPDATE THE README FILE
=============================================================
--> To Add From here
Coding up the Loss function

import numpy as np
import nnfs
from nnfs.datasets import spiral_data

nnfs.init()  # Setting up a default datatype for the numpy use

# Define the Layer_Dense class
class Layer_Dense:
    def __init__(self, n_inputs, n_neurons):
        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)
        self.biases = np.zeros((1, n_neurons))

    def forward(self, inputs):
        self.output = np.dot(inputs, self.weights) + self.biases

# Define the Activation_ReLU class
class Activation_ReLU:
    def forward(self, inputs):
        self.output = np.maximum(0, inputs)

class Activation_Softmax:
    def forward(self, inputs):
        exp_values = np.exp(inputs-np.max(inputs,axis=1,keepdims=True)) #avoiding overflow since we are using the exponential function
        probabilities = exp_values/np.sum(exp_values,axis=1,keepdims=True)
        self.output = probabilities

class Loss:
    def calculate(self, output, y):
        sample_losses = self.forward(output,y)
        data_loss = np.mean(sample_losses)
        return data_loss

class Loss_CategoricalCrossEntropy(Loss): #inherited from the Loss Class
    def forward(self, y_pred, y_true):
        samples = len(y_pred)
        y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)

        if len(y_true.shape)==1:
            correct_confidences = y_pred_clipped[range(samples), y_true]
        elif len(y_true.shape)==2:
            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)
        negative_log_likelihoods = -np.log(correct_confidences)
        return negative_log_likelihoods



X,y = spiral_data(samples=100,classes=3)
dense1 = Layer_Dense(2,3)
activation1 = Activation_ReLU()

dense2 = Layer_Dense(3,3)
activation2 = Activation_Softmax()

dense1.forward(X)
activation1.forward(dense1.output)

dense2.forward(activation1.output)
activation2.forward(dense2.output)

print(activation2.output[:5])
## GONNA HAVE TO UPDATE THE README FILE

loss_function = Loss_CategoricalCrossEntropy()
loss = loss_function.calculate(activation2.output, y)

print("loss:", loss)

## need to decrease the loss --> thats the goal